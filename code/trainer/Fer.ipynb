{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18777,"status":"ok","timestamp":1659888344395,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"},"user_tz":-420},"id":"cnD_0BGn8MzS","outputId":"1da1b8f4-2846-4e6a-c8b5-77fbddfee7b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":607,"status":"ok","timestamp":1659888344995,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"},"user_tz":-420},"id":"tIPi96DJJA5L","outputId":"58b42962-0a00-448d-8bed-f515b824ede8"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/WorkSpace/AI_Research/Emotion/code\n"]}],"source":["%cd /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/code/"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wVZznA6SJEQC","executionInfo":{"status":"ok","timestamp":1659888344997,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"}}},"outputs":[],"source":["import sys\n","sys.path.append(\"/content/drive/MyDrive/WorkSpace/AI_Research/Emotion/code/libs/\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zeFx2XNBJEnG","executionInfo":{"status":"ok","timestamp":1659888352950,"user_tz":-420,"elapsed":7959,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"36764068-b541-45b6-fd43-2f9ab330167c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1615: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1641: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n","  FutureWarning,\n"]}],"source":["import matplotlib.pyplot as plt\n","from sklearn.utils import shuffle\n","import pandas as pd\n","# from lr_finder import LRFinder\n","from utils.processing import data_processing\n","from utils import get_models\n","from utils import optimizers\n","from utils.datasets import data_generator\n","from utils.processing import data_augmentation\n","from utils import predict\n","from utils import attention_module\n","\n","import json\n","import numpy as np\n","import keras\n","import tensorflow as tf\n","from keras import layers, Input\n","from keras.utils import np_utils \n","\n","from tensorflow.keras.models import Sequential, model_from_json\n","from tensorflow.keras.applications import VGG16, ResNet50, VGG19, ResNet50V2, MobileNetV2, MobileNetV2\n","from tensorflow.keras.optimizers import Adam, Adamax\n","from keras.models import load_model, Model\n","\n"]},{"cell_type":"markdown","source":["## Load data"],"metadata":{"id":"mqBKK0L_XA-M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFumZPYRJjAj"},"outputs":[],"source":["# Get data\n","emotion = [\"Angry\" , \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\" , \"Neutral\"  ]\n","PATH_FER = \"/content/drive/MyDrive/WorkSpace/AI_Research/datasets/Fer/icml_face_data.csv\"\n","data = pd.read_csv(PATH_FER)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mlxah5Uofk-9"},"outputs":[],"source":["X, y = data_processing.get_fer(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWjITMcThjdv"},"outputs":[],"source":["X = np.array(X)\n","X = X.reshape((X.shape[0], 48, 48, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHnPytE98jzl"},"outputs":[],"source":["X_train,y_train, X_test, y_test, X_val, y_val = data_processing.load_fer(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RoT_0MQrTYo"},"outputs":[],"source":["# Offline augmentation\n","# datagen_val = data_augmentation.val_augmentation(X_val)\n","\n","\n","# X_train, y_train = data_augmentation.aug_offline(X_train, y_train, num_class = 1, number_aug = 7)\n","# X_train, y_train = data_augmentation.aug_offline(X_train, y_train, num_class = 5, number_aug =1)\n","\n","# X_train, y_train = shuffle(X_train, y_train)"]},{"cell_type":"code","source":["print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VcNuP8dDW9I9","executionInfo":{"status":"ok","timestamp":1655913871705,"user_tz":-420,"elapsed":25,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"}},"outputId":"4cb73633-11a0-401d-f370-259b03a0eb4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(28709, 48, 48, 1)\n","(3589, 48, 48, 1)\n","(3589, 48, 48, 1)\n"]}]},{"cell_type":"markdown","source":["## Data generator"],"metadata":{"id":"MdCXkLvuW9nw"}},{"cell_type":"code","source":["default_ws_dir =  \"/content/drive/MyDrive/WorkSpace/AI_Research/Emotion\"\n","configfer = json.load(open(f\"{default_ws_dir}/code/configs/ferplus_config.json\"))\n","config_train = json.load(open(f\"{default_ws_dir}/code/configs/wb_config.json\"))"],"metadata":{"id":"6OGtGRuZ0FUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_size =224"],"metadata":{"id":"iGg08eN_Wx-v","executionInfo":{"status":"ok","timestamp":1659888392218,"user_tz":-420,"elapsed":334,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khTp41EXKZsv"},"outputs":[],"source":["params_train = {\n","          'config': configfer,\n","          'shuffle': True,\n","          'augmentation': True}\n","params_val = {\n","          'config': configfer,\n","          'shuffle': None,\n","          'augmentation': None}         \n","\n","train_ds = data_generator.DataGenerator(X_train, y_train, **params_train ).to_dataset(data_type = \"train\")\n","val_ds = data_generator.DataGenerator(X_val, y_val, **params_val).to_dataset(data_type = \"val\")\n","test_ds = data_generator.DataGenerator(X_test, y_test, **params_val).to_dataset(data_type = \"val\", batch_size = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1655913872746,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"},"user_tz":-420},"id":"8sBBFnht2FNP","outputId":"27752576-78a6-4a37-b94c-5d0865cc21a4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7), dtype=tf.float32, name=None))>"]},"metadata":{},"execution_count":14}],"source":["val_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5peqUXpn0poi"},"outputs":[],"source":["batch1 , batch2 = val_ds.take(2)"]},{"cell_type":"code","source":["batch1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3wNF7joKRV9","executionInfo":{"status":"ok","timestamp":1655913872748,"user_tz":-420,"elapsed":14,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"}},"outputId":"446f2548-4b15-44ff-c4f6-497cc359508f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(32, 224, 224, 3), dtype=float32, numpy=\n"," array([[[[0.99607843, 0.99607843, 0.99607843],\n","          [0.99607843, 0.99607843, 0.99607843],\n","          [0.99607843, 0.99607843, 0.99607843],\n","          ...,\n","          [0.60784316, 0.60784316, 0.60784316],\n","          [0.6313726 , 0.6313726 , 0.6313726 ],\n","          [0.6313726 , 0.6313726 , 0.6313726 ]],\n"," \n","         [[0.99607843, 0.99607843, 0.99607843],\n","          [0.99607843, 0.99607843, 0.99607843],\n","          [0.99607843, 0.99607843, 0.99607843],\n","          ...,\n","          [0.60784316, 0.60784316, 0.60784316],\n","          [0.6313726 , 0.6313726 , 0.6313726 ],\n","          [0.6313726 , 0.6313726 , 0.6313726 ]],\n"," \n","         [[0.99607843, 0.99607843, 0.99607843],\n","          [0.99607843, 0.99607843, 0.99607843],\n","          [0.99607843, 0.99607843, 0.99607843],\n","          ...,\n","          [0.6039216 , 0.6039216 , 0.6039216 ],\n","          [0.627451  , 0.627451  , 0.627451  ],\n","          [0.627451  , 0.627451  , 0.627451  ]],\n"," \n","         ...,\n"," \n","         [[0.36078432, 0.36078432, 0.36078432],\n","          [0.36078432, 0.36078432, 0.36078432],\n","          [0.3647059 , 0.3647059 , 0.3647059 ],\n","          ...,\n","          [0.70980394, 0.70980394, 0.70980394],\n","          [0.7176471 , 0.7176471 , 0.7176471 ],\n","          [0.7176471 , 0.7176471 , 0.7176471 ]],\n"," \n","         [[0.3647059 , 0.3647059 , 0.3647059 ],\n","          [0.3647059 , 0.3647059 , 0.3647059 ],\n","          [0.3647059 , 0.3647059 , 0.3647059 ],\n","          ...,\n","          [0.69803923, 0.69803923, 0.69803923],\n","          [0.7058824 , 0.7058824 , 0.7058824 ],\n","          [0.7058824 , 0.7058824 , 0.7058824 ]],\n"," \n","         [[0.3647059 , 0.3647059 , 0.3647059 ],\n","          [0.3647059 , 0.3647059 , 0.3647059 ],\n","          [0.3647059 , 0.3647059 , 0.3647059 ],\n","          ...,\n","          [0.69803923, 0.69803923, 0.69803923],\n","          [0.7058824 , 0.7058824 , 0.7058824 ],\n","          [0.7058824 , 0.7058824 , 0.7058824 ]]],\n"," \n"," \n","        [[[0.6117647 , 0.6117647 , 0.6117647 ],\n","          [0.6117647 , 0.6117647 , 0.6117647 ],\n","          [0.6156863 , 0.6156863 , 0.6156863 ],\n","          ...,\n","          [0.5882353 , 0.5882353 , 0.5882353 ],\n","          [0.5882353 , 0.5882353 , 0.5882353 ],\n","          [0.5882353 , 0.5882353 , 0.5882353 ]],\n"," \n","         [[0.6117647 , 0.6117647 , 0.6117647 ],\n","          [0.6117647 , 0.6117647 , 0.6117647 ],\n","          [0.6156863 , 0.6156863 , 0.6156863 ],\n","          ...,\n","          [0.5882353 , 0.5882353 , 0.5882353 ],\n","          [0.5882353 , 0.5882353 , 0.5882353 ],\n","          [0.5882353 , 0.5882353 , 0.5882353 ]],\n"," \n","         [[0.60784316, 0.60784316, 0.60784316],\n","          [0.60784316, 0.60784316, 0.60784316],\n","          [0.6117647 , 0.6117647 , 0.6117647 ],\n","          ...,\n","          [0.5882353 , 0.5882353 , 0.5882353 ],\n","          [0.5882353 , 0.5882353 , 0.5882353 ],\n","          [0.5882353 , 0.5882353 , 0.5882353 ]],\n"," \n","         ...,\n"," \n","         [[0.08235294, 0.08235294, 0.08235294],\n","          [0.08235294, 0.08235294, 0.08235294],\n","          [0.08235294, 0.08235294, 0.08235294],\n","          ...,\n","          [0.6313726 , 0.6313726 , 0.6313726 ],\n","          [0.6313726 , 0.6313726 , 0.6313726 ],\n","          [0.6313726 , 0.6313726 , 0.6313726 ]],\n"," \n","         [[0.08235294, 0.08235294, 0.08235294],\n","          [0.08235294, 0.08235294, 0.08235294],\n","          [0.08235294, 0.08235294, 0.08235294],\n","          ...,\n","          [0.6313726 , 0.6313726 , 0.6313726 ],\n","          [0.6313726 , 0.6313726 , 0.6313726 ],\n","          [0.6313726 , 0.6313726 , 0.6313726 ]],\n"," \n","         [[0.08235294, 0.08235294, 0.08235294],\n","          [0.08235294, 0.08235294, 0.08235294],\n","          [0.08235294, 0.08235294, 0.08235294],\n","          ...,\n","          [0.6313726 , 0.6313726 , 0.6313726 ],\n","          [0.6313726 , 0.6313726 , 0.6313726 ],\n","          [0.6313726 , 0.6313726 , 0.6313726 ]]],\n"," \n"," \n","        [[[0.27058825, 0.27058825, 0.27058825],\n","          [0.27058825, 0.27058825, 0.27058825],\n","          [0.2784314 , 0.2784314 , 0.2784314 ],\n","          ...,\n","          [0.56078434, 0.56078434, 0.56078434],\n","          [0.5647059 , 0.5647059 , 0.5647059 ],\n","          [0.5647059 , 0.5647059 , 0.5647059 ]],\n"," \n","         [[0.27058825, 0.27058825, 0.27058825],\n","          [0.27058825, 0.27058825, 0.27058825],\n","          [0.2784314 , 0.2784314 , 0.2784314 ],\n","          ...,\n","          [0.56078434, 0.56078434, 0.56078434],\n","          [0.5647059 , 0.5647059 , 0.5647059 ],\n","          [0.5647059 , 0.5647059 , 0.5647059 ]],\n"," \n","         [[0.27058825, 0.27058825, 0.27058825],\n","          [0.27058825, 0.27058825, 0.27058825],\n","          [0.27450982, 0.27450982, 0.27450982],\n","          ...,\n","          [0.56078434, 0.56078434, 0.56078434],\n","          [0.56078434, 0.56078434, 0.56078434],\n","          [0.56078434, 0.56078434, 0.56078434]],\n"," \n","         ...,\n"," \n","         [[0.44705883, 0.44705883, 0.44705883],\n","          [0.44705883, 0.44705883, 0.44705883],\n","          [0.44313726, 0.44313726, 0.44313726],\n","          ...,\n","          [0.3529412 , 0.3529412 , 0.3529412 ],\n","          [0.3529412 , 0.3529412 , 0.3529412 ],\n","          [0.3529412 , 0.3529412 , 0.3529412 ]],\n"," \n","         [[0.44705883, 0.44705883, 0.44705883],\n","          [0.44705883, 0.44705883, 0.44705883],\n","          [0.44313726, 0.44313726, 0.44313726],\n","          ...,\n","          [0.3529412 , 0.3529412 , 0.3529412 ],\n","          [0.3529412 , 0.3529412 , 0.3529412 ],\n","          [0.3529412 , 0.3529412 , 0.3529412 ]],\n"," \n","         [[0.44705883, 0.44705883, 0.44705883],\n","          [0.44705883, 0.44705883, 0.44705883],\n","          [0.44313726, 0.44313726, 0.44313726],\n","          ...,\n","          [0.3529412 , 0.3529412 , 0.3529412 ],\n","          [0.3529412 , 0.3529412 , 0.3529412 ],\n","          [0.3529412 , 0.3529412 , 0.3529412 ]]],\n"," \n"," \n","        ...,\n"," \n"," \n","        [[[0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          ...,\n","          [0.4627451 , 0.4627451 , 0.4627451 ],\n","          [0.4627451 , 0.4627451 , 0.4627451 ],\n","          [0.4627451 , 0.4627451 , 0.4627451 ]],\n"," \n","         [[0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          ...,\n","          [0.4627451 , 0.4627451 , 0.4627451 ],\n","          [0.4627451 , 0.4627451 , 0.4627451 ],\n","          [0.4627451 , 0.4627451 , 0.4627451 ]],\n"," \n","         [[0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          ...,\n","          [0.4627451 , 0.4627451 , 0.4627451 ],\n","          [0.4627451 , 0.4627451 , 0.4627451 ],\n","          [0.4627451 , 0.4627451 , 0.4627451 ]],\n"," \n","         ...,\n"," \n","         [[0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          ...,\n","          [0.21176471, 0.21176471, 0.21176471],\n","          [0.21176471, 0.21176471, 0.21176471],\n","          [0.21176471, 0.21176471, 0.21176471]],\n"," \n","         [[0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          ...,\n","          [0.21176471, 0.21176471, 0.21176471],\n","          [0.21176471, 0.21176471, 0.21176471],\n","          [0.21176471, 0.21176471, 0.21176471]],\n"," \n","         [[0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          [0.9882353 , 0.9882353 , 0.9882353 ],\n","          ...,\n","          [0.21176471, 0.21176471, 0.21176471],\n","          [0.21176471, 0.21176471, 0.21176471],\n","          [0.21176471, 0.21176471, 0.21176471]]],\n"," \n"," \n","        [[[0.41568628, 0.41568628, 0.41568628],\n","          [0.41568628, 0.41568628, 0.41568628],\n","          [0.41568628, 0.41568628, 0.41568628],\n","          ...,\n","          [0.6039216 , 0.6039216 , 0.6039216 ],\n","          [0.6039216 , 0.6039216 , 0.6039216 ],\n","          [0.6039216 , 0.6039216 , 0.6039216 ]],\n"," \n","         [[0.41568628, 0.41568628, 0.41568628],\n","          [0.41568628, 0.41568628, 0.41568628],\n","          [0.41568628, 0.41568628, 0.41568628],\n","          ...,\n","          [0.6039216 , 0.6039216 , 0.6039216 ],\n","          [0.6039216 , 0.6039216 , 0.6039216 ],\n","          [0.6039216 , 0.6039216 , 0.6039216 ]],\n"," \n","         [[0.41568628, 0.41568628, 0.41568628],\n","          [0.41568628, 0.41568628, 0.41568628],\n","          [0.41568628, 0.41568628, 0.41568628],\n","          ...,\n","          [0.6       , 0.6       , 0.6       ],\n","          [0.6       , 0.6       , 0.6       ],\n","          [0.6       , 0.6       , 0.6       ]],\n"," \n","         ...,\n"," \n","         [[0.48235294, 0.48235294, 0.48235294],\n","          [0.48235294, 0.48235294, 0.48235294],\n","          [0.48235294, 0.48235294, 0.48235294],\n","          ...,\n","          [0.78431374, 0.78431374, 0.78431374],\n","          [0.78431374, 0.78431374, 0.78431374],\n","          [0.78431374, 0.78431374, 0.78431374]],\n"," \n","         [[0.4745098 , 0.4745098 , 0.4745098 ],\n","          [0.4745098 , 0.4745098 , 0.4745098 ],\n","          [0.47058824, 0.47058824, 0.47058824],\n","          ...,\n","          [0.78431374, 0.78431374, 0.78431374],\n","          [0.78431374, 0.78431374, 0.78431374],\n","          [0.78431374, 0.78431374, 0.78431374]],\n"," \n","         [[0.4745098 , 0.4745098 , 0.4745098 ],\n","          [0.4745098 , 0.4745098 , 0.4745098 ],\n","          [0.47058824, 0.47058824, 0.47058824],\n","          ...,\n","          [0.78431374, 0.78431374, 0.78431374],\n","          [0.78431374, 0.78431374, 0.78431374],\n","          [0.78431374, 0.78431374, 0.78431374]]],\n"," \n"," \n","        [[[0.23137255, 0.23137255, 0.23137255],\n","          [0.23137255, 0.23137255, 0.23137255],\n","          [0.23137255, 0.23137255, 0.23137255],\n","          ...,\n","          [0.22745098, 0.22745098, 0.22745098],\n","          [0.22745098, 0.22745098, 0.22745098],\n","          [0.22745098, 0.22745098, 0.22745098]],\n"," \n","         [[0.23137255, 0.23137255, 0.23137255],\n","          [0.23137255, 0.23137255, 0.23137255],\n","          [0.23137255, 0.23137255, 0.23137255],\n","          ...,\n","          [0.22745098, 0.22745098, 0.22745098],\n","          [0.22745098, 0.22745098, 0.22745098],\n","          [0.22745098, 0.22745098, 0.22745098]],\n"," \n","         [[0.23137255, 0.23137255, 0.23137255],\n","          [0.23137255, 0.23137255, 0.23137255],\n","          [0.23137255, 0.23137255, 0.23137255],\n","          ...,\n","          [0.22745098, 0.22745098, 0.22745098],\n","          [0.22745098, 0.22745098, 0.22745098],\n","          [0.22745098, 0.22745098, 0.22745098]],\n"," \n","         ...,\n"," \n","         [[0.9647059 , 0.9647059 , 0.9647059 ],\n","          [0.9647059 , 0.9647059 , 0.9647059 ],\n","          [0.9647059 , 0.9647059 , 0.9647059 ],\n","          ...,\n","          [0.9137255 , 0.9137255 , 0.9137255 ],\n","          [0.9137255 , 0.9137255 , 0.9137255 ],\n","          [0.9137255 , 0.9137255 , 0.9137255 ]],\n"," \n","         [[0.9647059 , 0.9647059 , 0.9647059 ],\n","          [0.9647059 , 0.9647059 , 0.9647059 ],\n","          [0.9647059 , 0.9647059 , 0.9647059 ],\n","          ...,\n","          [0.9137255 , 0.9137255 , 0.9137255 ],\n","          [0.9137255 , 0.9137255 , 0.9137255 ],\n","          [0.9137255 , 0.9137255 , 0.9137255 ]],\n"," \n","         [[0.9647059 , 0.9647059 , 0.9647059 ],\n","          [0.9647059 , 0.9647059 , 0.9647059 ],\n","          [0.9647059 , 0.9647059 , 0.9647059 ],\n","          ...,\n","          [0.9137255 , 0.9137255 , 0.9137255 ],\n","          [0.9137255 , 0.9137255 , 0.9137255 ],\n","          [0.9137255 , 0.9137255 , 0.9137255 ]]]], dtype=float32)>,\n"," <tf.Tensor: shape=(32, 7), dtype=float32, numpy=\n"," array([[1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 1., 0., 0., 0.],\n","        [0., 0., 0., 1., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 1., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 1., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 1., 0.],\n","        [0., 0., 0., 1., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 1., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 1., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"dlXa3CbHNbFT","executionInfo":{"status":"ok","timestamp":1659888397154,"user_tz":-420,"elapsed":332,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"}}},"outputs":[],"source":["# Create input shape\n","img_size_target = img_size\n","img_input = Input(shape=(img_size_target, img_size_target, 1))\n","img_conc = layers.Concatenate()([img_input, img_input, img_input])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhwehF7RNr3e"},"outputs":[],"source":["model = get_models.Pre_train_model(VGG16, None, (224,224,3))\n","\n","# model.load_weights(\"/content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/FerPlusV11.hdf5\")\n","model.compile(loss =config_train[\"loss\"], optimizer =Adam(0.0001), metrics=[config_train[\"metric\"]])\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=tf.math.exp(-0.1),\n","                              patience=3, min_lr=0 , verbose =1)\n","\n","files = configfer[\"file_checkpoint\"]\n","checkpoint_filepath = (f\"{default_ws_dir}/checkpoints/{files}\")\n","\n","model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    verbose= 1,\n","    monitor='val_accuracy',\n","    mode='auto',\n","    save_best_only=True, save_weights_only = True)"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"d0rwsJzrvZ2r"}},{"cell_type":"code","source":["class_weights = {\n","    0 : 1.02660468,\n","    1 : 9.40661861,\n","    2 : 1.00104606,\n","    3 : 0.56843877,\n","    4 : 0.84912748,\n","    5 : 1.29337298,\n","    6 : 0.82603942,\n","}"],"metadata":{"id":"3U7fAYrzzQaS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aLJqg0bIu0y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3463e53f-7215-467a-c242-e620cbbfec71","executionInfo":{"status":"ok","timestamp":1655955025998,"user_tz":-420,"elapsed":30797423,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","1037/1037 [==============================] - ETA: 0s - loss: 2.0711 - accuracy: 0.2382\n","Epoch 1: val_accuracy improved from -inf to 0.31903, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 426s 398ms/step - loss: 2.0711 - accuracy: 0.2382 - val_loss: 1.8423 - val_accuracy: 0.3190 - lr: 1.0000e-04\n","Epoch 2/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.7440 - accuracy: 0.3692\n","Epoch 2: val_accuracy improved from 0.31903 to 0.44943, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 410s 395ms/step - loss: 1.7440 - accuracy: 0.3692 - val_loss: 1.5991 - val_accuracy: 0.4494 - lr: 1.0000e-04\n","Epoch 3/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.5749 - accuracy: 0.4431\n","Epoch 3: val_accuracy improved from 0.44943 to 0.51658, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 409s 395ms/step - loss: 1.5749 - accuracy: 0.4431 - val_loss: 1.4378 - val_accuracy: 0.5166 - lr: 1.0000e-04\n","Epoch 4/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.4880 - accuracy: 0.4823\n","Epoch 4: val_accuracy did not improve from 0.51658\n","1037/1037 [==============================] - 409s 395ms/step - loss: 1.4880 - accuracy: 0.4823 - val_loss: 1.4145 - val_accuracy: 0.5010 - lr: 1.0000e-04\n","Epoch 5/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.3993 - accuracy: 0.5194\n","Epoch 5: val_accuracy improved from 0.51658 to 0.54834, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 411s 396ms/step - loss: 1.3993 - accuracy: 0.5194 - val_loss: 1.3228 - val_accuracy: 0.5483 - lr: 1.0000e-04\n","Epoch 6/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.3259 - accuracy: 0.5463\n","Epoch 6: val_accuracy improved from 0.54834 to 0.56478, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 412s 398ms/step - loss: 1.3259 - accuracy: 0.5463 - val_loss: 1.2567 - val_accuracy: 0.5648 - lr: 1.0000e-04\n","Epoch 7/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.2754 - accuracy: 0.5643\n","Epoch 7: val_accuracy did not improve from 0.56478\n","1037/1037 [==============================] - 407s 392ms/step - loss: 1.2754 - accuracy: 0.5643 - val_loss: 1.2952 - val_accuracy: 0.5550 - lr: 1.0000e-04\n","Epoch 8/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.2286 - accuracy: 0.5798\n","Epoch 8: val_accuracy improved from 0.56478 to 0.59097, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 408s 393ms/step - loss: 1.2286 - accuracy: 0.5798 - val_loss: 1.2022 - val_accuracy: 0.5910 - lr: 1.0000e-04\n","Epoch 9/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.1733 - accuracy: 0.6031\n","Epoch 9: val_accuracy improved from 0.59097 to 0.60741, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 408s 394ms/step - loss: 1.1733 - accuracy: 0.6031 - val_loss: 1.1518 - val_accuracy: 0.6074 - lr: 1.0000e-04\n","Epoch 10/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.1365 - accuracy: 0.6161\n","Epoch 10: val_accuracy improved from 0.60741 to 0.60853, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 410s 395ms/step - loss: 1.1365 - accuracy: 0.6161 - val_loss: 1.1440 - val_accuracy: 0.6085 - lr: 1.0000e-04\n","Epoch 11/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.1025 - accuracy: 0.6327\n","Epoch 11: val_accuracy improved from 0.60853 to 0.61605, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 409s 394ms/step - loss: 1.1025 - accuracy: 0.6327 - val_loss: 1.1217 - val_accuracy: 0.6160 - lr: 1.0000e-04\n","Epoch 12/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.0583 - accuracy: 0.6489\n","Epoch 12: val_accuracy did not improve from 0.61605\n","1037/1037 [==============================] - 409s 394ms/step - loss: 1.0583 - accuracy: 0.6489 - val_loss: 1.1761 - val_accuracy: 0.6043 - lr: 1.0000e-04\n","Epoch 13/100\n","1037/1037 [==============================] - ETA: 0s - loss: 1.0300 - accuracy: 0.6565\n","Epoch 13: val_accuracy improved from 0.61605 to 0.61688, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 408s 394ms/step - loss: 1.0300 - accuracy: 0.6565 - val_loss: 1.1522 - val_accuracy: 0.6169 - lr: 1.0000e-04\n","Epoch 14/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.9975 - accuracy: 0.6692\n","Epoch 14: val_accuracy improved from 0.61688 to 0.63054, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 408s 394ms/step - loss: 0.9975 - accuracy: 0.6692 - val_loss: 1.1013 - val_accuracy: 0.6305 - lr: 1.0000e-04\n","Epoch 15/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.9598 - accuracy: 0.6857\n","Epoch 15: val_accuracy did not improve from 0.63054\n","1037/1037 [==============================] - 408s 393ms/step - loss: 0.9598 - accuracy: 0.6857 - val_loss: 1.1039 - val_accuracy: 0.6283 - lr: 1.0000e-04\n","Epoch 16/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.9347 - accuracy: 0.6924\n","Epoch 16: val_accuracy improved from 0.63054 to 0.64224, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 409s 394ms/step - loss: 0.9347 - accuracy: 0.6924 - val_loss: 1.0930 - val_accuracy: 0.6422 - lr: 1.0000e-04\n","Epoch 17/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.8989 - accuracy: 0.7058\n","Epoch 17: val_accuracy did not improve from 0.64224\n","1037/1037 [==============================] - 409s 394ms/step - loss: 0.8989 - accuracy: 0.7058 - val_loss: 1.1443 - val_accuracy: 0.6291 - lr: 1.0000e-04\n","Epoch 18/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.8672 - accuracy: 0.7157\n","Epoch 18: val_accuracy did not improve from 0.64224\n","1037/1037 [==============================] - 408s 393ms/step - loss: 0.8672 - accuracy: 0.7157 - val_loss: 1.1449 - val_accuracy: 0.6286 - lr: 1.0000e-04\n","Epoch 19/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.8343 - accuracy: 0.7331\n","Epoch 19: ReduceLROnPlateau reducing learning rate to 9.048373613040894e-05.\n","\n","Epoch 19: val_accuracy improved from 0.64224 to 0.64391, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.8343 - accuracy: 0.7331 - val_loss: 1.1686 - val_accuracy: 0.6439 - lr: 1.0000e-04\n","Epoch 20/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.7907 - accuracy: 0.7451\n","Epoch 20: val_accuracy did not improve from 0.64391\n","1037/1037 [==============================] - 408s 394ms/step - loss: 0.7907 - accuracy: 0.7451 - val_loss: 1.1301 - val_accuracy: 0.6408 - lr: 9.0484e-05\n","Epoch 21/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.7554 - accuracy: 0.7584\n","Epoch 21: val_accuracy did not improve from 0.64391\n","1037/1037 [==============================] - 409s 394ms/step - loss: 0.7554 - accuracy: 0.7584 - val_loss: 1.1999 - val_accuracy: 0.6406 - lr: 9.0484e-05\n","Epoch 22/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.7212 - accuracy: 0.7752\n","Epoch 22: ReduceLROnPlateau reducing learning rate to 8.187306229956448e-05.\n","\n","Epoch 22: val_accuracy improved from 0.64391 to 0.65422, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.7212 - accuracy: 0.7752 - val_loss: 1.1482 - val_accuracy: 0.6542 - lr: 9.0484e-05\n","Epoch 23/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.6814 - accuracy: 0.7841\n","Epoch 23: val_accuracy improved from 0.65422 to 0.65561, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 410s 395ms/step - loss: 0.6814 - accuracy: 0.7841 - val_loss: 1.1899 - val_accuracy: 0.6556 - lr: 8.1873e-05\n","Epoch 24/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.7993\n","Epoch 24: val_accuracy did not improve from 0.65561\n","1037/1037 [==============================] - 408s 394ms/step - loss: 0.6474 - accuracy: 0.7993 - val_loss: 1.1733 - val_accuracy: 0.6537 - lr: 8.1873e-05\n","Epoch 25/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.6151 - accuracy: 0.8118\n","Epoch 25: ReduceLROnPlateau reducing learning rate to 7.408180681522936e-05.\n","\n","Epoch 25: val_accuracy improved from 0.65561 to 0.65589, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 424s 409ms/step - loss: 0.6151 - accuracy: 0.8118 - val_loss: 1.2308 - val_accuracy: 0.6559 - lr: 8.1873e-05\n","Epoch 26/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.8236\n","Epoch 26: val_accuracy did not improve from 0.65589\n","1037/1037 [==============================] - 398s 384ms/step - loss: 0.5805 - accuracy: 0.8236 - val_loss: 1.3802 - val_accuracy: 0.6389 - lr: 7.4082e-05\n","Epoch 27/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.5560 - accuracy: 0.8328\n","Epoch 27: val_accuracy improved from 0.65589 to 0.66342, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.5560 - accuracy: 0.8328 - val_loss: 1.3570 - val_accuracy: 0.6634 - lr: 7.4082e-05\n","Epoch 28/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.8451\n","Epoch 28: ReduceLROnPlateau reducing learning rate to 6.703198596369475e-05.\n","\n","Epoch 28: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 408s 393ms/step - loss: 0.5270 - accuracy: 0.8451 - val_loss: 1.3890 - val_accuracy: 0.6609 - lr: 7.4082e-05\n","Epoch 29/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.4884 - accuracy: 0.8568\n","Epoch 29: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 407s 393ms/step - loss: 0.4884 - accuracy: 0.8568 - val_loss: 1.3122 - val_accuracy: 0.6548 - lr: 6.7032e-05\n","Epoch 30/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.4661 - accuracy: 0.8667\n","Epoch 30: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 408s 394ms/step - loss: 0.4661 - accuracy: 0.8667 - val_loss: 1.5482 - val_accuracy: 0.6317 - lr: 6.7032e-05\n","Epoch 31/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.4415 - accuracy: 0.8735\n","Epoch 31: ReduceLROnPlateau reducing learning rate to 6.065304478397593e-05.\n","\n","Epoch 31: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.4415 - accuracy: 0.8735 - val_loss: 1.4200 - val_accuracy: 0.6467 - lr: 6.7032e-05\n","Epoch 32/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.4107 - accuracy: 0.8841\n","Epoch 32: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 407s 393ms/step - loss: 0.4107 - accuracy: 0.8841 - val_loss: 1.4593 - val_accuracy: 0.6584 - lr: 6.0653e-05\n","Epoch 33/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.8890\n","Epoch 33: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 408s 393ms/step - loss: 0.3898 - accuracy: 0.8890 - val_loss: 1.3742 - val_accuracy: 0.6581 - lr: 6.0653e-05\n","Epoch 34/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.8955\n","Epoch 34: ReduceLROnPlateau reducing learning rate to 5.488114038598724e-05.\n","\n","Epoch 34: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.3773 - accuracy: 0.8955 - val_loss: 1.5843 - val_accuracy: 0.6570 - lr: 6.0653e-05\n","Epoch 35/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.3513 - accuracy: 0.9041\n","Epoch 35: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 409s 394ms/step - loss: 0.3513 - accuracy: 0.9041 - val_loss: 1.5239 - val_accuracy: 0.6531 - lr: 5.4881e-05\n","Epoch 36/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.9095\n","Epoch 36: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 408s 394ms/step - loss: 0.3336 - accuracy: 0.9095 - val_loss: 1.5033 - val_accuracy: 0.6539 - lr: 5.4881e-05\n","Epoch 37/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.9164\n","Epoch 37: ReduceLROnPlateau reducing learning rate to 4.9658505304250866e-05.\n","\n","Epoch 37: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.3167 - accuracy: 0.9164 - val_loss: 1.8059 - val_accuracy: 0.6495 - lr: 5.4881e-05\n","Epoch 38/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.9223\n","Epoch 38: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 409s 394ms/step - loss: 0.2993 - accuracy: 0.9223 - val_loss: 1.6321 - val_accuracy: 0.6562 - lr: 4.9659e-05\n","Epoch 39/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.9284\n","Epoch 39: val_accuracy did not improve from 0.66342\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.2800 - accuracy: 0.9284 - val_loss: 1.5690 - val_accuracy: 0.6592 - lr: 4.9659e-05\n","Epoch 40/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.9296\n","Epoch 40: ReduceLROnPlateau reducing learning rate to 4.493287269724533e-05.\n","\n","Epoch 40: val_accuracy improved from 0.66342 to 0.67066, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.2751 - accuracy: 0.9296 - val_loss: 1.6061 - val_accuracy: 0.6707 - lr: 4.9659e-05\n","Epoch 41/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.2553 - accuracy: 0.9370\n","Epoch 41: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 395ms/step - loss: 0.2553 - accuracy: 0.9370 - val_loss: 1.7468 - val_accuracy: 0.6565 - lr: 4.4933e-05\n","Epoch 42/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.9385\n","Epoch 42: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.2462 - accuracy: 0.9385 - val_loss: 1.7504 - val_accuracy: 0.6539 - lr: 4.4933e-05\n","Epoch 43/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.9444\n","Epoch 43: ReduceLROnPlateau reducing learning rate to 4.065694156452082e-05.\n","\n","Epoch 43: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 408s 394ms/step - loss: 0.2315 - accuracy: 0.9444 - val_loss: 1.8687 - val_accuracy: 0.6498 - lr: 4.4933e-05\n","Epoch 44/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.2168 - accuracy: 0.9490\n","Epoch 44: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 395ms/step - loss: 0.2168 - accuracy: 0.9490 - val_loss: 2.0888 - val_accuracy: 0.6464 - lr: 4.0657e-05\n","Epoch 45/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9490\n","Epoch 45: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 408s 394ms/step - loss: 0.2155 - accuracy: 0.9490 - val_loss: 1.8160 - val_accuracy: 0.6545 - lr: 4.0657e-05\n","Epoch 46/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9520\n","Epoch 46: ReduceLROnPlateau reducing learning rate to 3.678791836136952e-05.\n","\n","Epoch 46: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 411s 396ms/step - loss: 0.2061 - accuracy: 0.9520 - val_loss: 1.7779 - val_accuracy: 0.6562 - lr: 4.0657e-05\n","Epoch 47/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9569\n","Epoch 47: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.1916 - accuracy: 0.9569 - val_loss: 1.8603 - val_accuracy: 0.6615 - lr: 3.6788e-05\n","Epoch 48/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1858 - accuracy: 0.9584\n","Epoch 48: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 396ms/step - loss: 0.1858 - accuracy: 0.9584 - val_loss: 1.8754 - val_accuracy: 0.6670 - lr: 3.6788e-05\n","Epoch 49/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1752 - accuracy: 0.9614\n","Epoch 49: ReduceLROnPlateau reducing learning rate to 3.328708407934755e-05.\n","\n","Epoch 49: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 409s 394ms/step - loss: 0.1752 - accuracy: 0.9614 - val_loss: 1.9863 - val_accuracy: 0.6553 - lr: 3.6788e-05\n","Epoch 50/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9642\n","Epoch 50: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 396ms/step - loss: 0.1672 - accuracy: 0.9642 - val_loss: 2.0214 - val_accuracy: 0.6643 - lr: 3.3287e-05\n","Epoch 51/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9654\n","Epoch 51: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 395ms/step - loss: 0.1647 - accuracy: 0.9654 - val_loss: 2.0075 - val_accuracy: 0.6679 - lr: 3.3287e-05\n","Epoch 52/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1578 - accuracy: 0.9676\n","Epoch 52: ReduceLROnPlateau reducing learning rate to 3.0119397706585005e-05.\n","\n","Epoch 52: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 395ms/step - loss: 0.1578 - accuracy: 0.9676 - val_loss: 2.0365 - val_accuracy: 0.6623 - lr: 3.3287e-05\n","Epoch 53/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9704\n","Epoch 53: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 396ms/step - loss: 0.1502 - accuracy: 0.9704 - val_loss: 2.0153 - val_accuracy: 0.6682 - lr: 3.0119e-05\n","Epoch 54/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9715\n","Epoch 54: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 409s 395ms/step - loss: 0.1417 - accuracy: 0.9715 - val_loss: 2.2323 - val_accuracy: 0.6534 - lr: 3.0119e-05\n","Epoch 55/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9728\n","Epoch 55: ReduceLROnPlateau reducing learning rate to 2.7253156076767482e-05.\n","\n","Epoch 55: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 412s 398ms/step - loss: 0.1422 - accuracy: 0.9728 - val_loss: 2.0497 - val_accuracy: 0.6651 - lr: 3.0119e-05\n","Epoch 56/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9742\n","Epoch 56: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 395ms/step - loss: 0.1337 - accuracy: 0.9742 - val_loss: 2.1485 - val_accuracy: 0.6612 - lr: 2.7253e-05\n","Epoch 57/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9752\n","Epoch 57: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 396ms/step - loss: 0.1316 - accuracy: 0.9752 - val_loss: 2.2435 - val_accuracy: 0.6551 - lr: 2.7253e-05\n","Epoch 58/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9756\n","Epoch 58: ReduceLROnPlateau reducing learning rate to 2.4659673727001064e-05.\n","\n","Epoch 58: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 395ms/step - loss: 0.1263 - accuracy: 0.9756 - val_loss: 2.1824 - val_accuracy: 0.6581 - lr: 2.7253e-05\n","Epoch 59/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1248 - accuracy: 0.9777\n","Epoch 59: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 410s 396ms/step - loss: 0.1248 - accuracy: 0.9777 - val_loss: 2.1674 - val_accuracy: 0.6654 - lr: 2.4660e-05\n","Epoch 60/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9784\n","Epoch 60: val_accuracy did not improve from 0.67066\n","1037/1037 [==============================] - 411s 396ms/step - loss: 0.1183 - accuracy: 0.9784 - val_loss: 2.2588 - val_accuracy: 0.6662 - lr: 2.4660e-05\n","Epoch 61/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1151 - accuracy: 0.9795\n","Epoch 61: ReduceLROnPlateau reducing learning rate to 2.231299367849715e-05.\n","\n","Epoch 61: val_accuracy improved from 0.67066 to 0.67177, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 411s 397ms/step - loss: 0.1151 - accuracy: 0.9795 - val_loss: 2.1676 - val_accuracy: 0.6718 - lr: 2.4660e-05\n","Epoch 62/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9807\n","Epoch 62: val_accuracy improved from 0.67177 to 0.67261, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 414s 399ms/step - loss: 0.1081 - accuracy: 0.9807 - val_loss: 2.1783 - val_accuracy: 0.6726 - lr: 2.2313e-05\n","Epoch 63/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9809\n","Epoch 63: val_accuracy did not improve from 0.67261\n","1037/1037 [==============================] - 410s 395ms/step - loss: 0.1103 - accuracy: 0.9809 - val_loss: 2.2312 - val_accuracy: 0.6643 - lr: 2.2313e-05\n","Epoch 64/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9824\n","Epoch 64: ReduceLROnPlateau reducing learning rate to 2.0189630959066562e-05.\n","\n","Epoch 64: val_accuracy did not improve from 0.67261\n","1037/1037 [==============================] - 410s 395ms/step - loss: 0.1058 - accuracy: 0.9824 - val_loss: 2.3212 - val_accuracy: 0.6590 - lr: 2.2313e-05\n","Epoch 65/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9827\n","Epoch 65: val_accuracy did not improve from 0.67261\n","1037/1037 [==============================] - 411s 397ms/step - loss: 0.1043 - accuracy: 0.9827 - val_loss: 2.2720 - val_accuracy: 0.6670 - lr: 2.0190e-05\n","Epoch 66/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9845\n","Epoch 66: val_accuracy did not improve from 0.67261\n","1037/1037 [==============================] - 412s 397ms/step - loss: 0.0979 - accuracy: 0.9845 - val_loss: 2.1750 - val_accuracy: 0.6723 - lr: 2.0190e-05\n","Epoch 67/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9838\n","Epoch 67: ReduceLROnPlateau reducing learning rate to 1.826833249651827e-05.\n","\n","Epoch 67: val_accuracy did not improve from 0.67261\n","1037/1037 [==============================] - 416s 401ms/step - loss: 0.0980 - accuracy: 0.9838 - val_loss: 2.3544 - val_accuracy: 0.6690 - lr: 2.0190e-05\n","Epoch 68/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9842\n","Epoch 68: val_accuracy improved from 0.67261 to 0.67317, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 416s 402ms/step - loss: 0.0978 - accuracy: 0.9842 - val_loss: 2.1158 - val_accuracy: 0.6732 - lr: 1.8268e-05\n","Epoch 69/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9865\n","Epoch 69: val_accuracy did not improve from 0.67317\n","1037/1037 [==============================] - 412s 398ms/step - loss: 0.0898 - accuracy: 0.9865 - val_loss: 2.1804 - val_accuracy: 0.6656 - lr: 1.8268e-05\n","Epoch 70/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9870\n","Epoch 70: ReduceLROnPlateau reducing learning rate to 1.6529869753867388e-05.\n","\n","Epoch 70: val_accuracy did not improve from 0.67317\n","1037/1037 [==============================] - 415s 401ms/step - loss: 0.0908 - accuracy: 0.9870 - val_loss: 2.3174 - val_accuracy: 0.6645 - lr: 1.8268e-05\n","Epoch 71/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9878\n","Epoch 71: val_accuracy did not improve from 0.67317\n","1037/1037 [==============================] - 416s 401ms/step - loss: 0.0870 - accuracy: 0.9878 - val_loss: 2.3473 - val_accuracy: 0.6693 - lr: 1.6530e-05\n","Epoch 72/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9881\n","Epoch 72: val_accuracy did not improve from 0.67317\n","1037/1037 [==============================] - 415s 400ms/step - loss: 0.0834 - accuracy: 0.9881 - val_loss: 2.3353 - val_accuracy: 0.6721 - lr: 1.6530e-05\n","Epoch 73/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9866\n","Epoch 73: ReduceLROnPlateau reducing learning rate to 1.4956844097469002e-05.\n","\n","Epoch 73: val_accuracy did not improve from 0.67317\n","1037/1037 [==============================] - 418s 403ms/step - loss: 0.0894 - accuracy: 0.9866 - val_loss: 2.2922 - val_accuracy: 0.6723 - lr: 1.6530e-05\n","Epoch 74/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9879\n","Epoch 74: val_accuracy did not improve from 0.67317\n","1037/1037 [==============================] - 416s 402ms/step - loss: 0.0841 - accuracy: 0.9879 - val_loss: 2.1999 - val_accuracy: 0.6665 - lr: 1.4957e-05\n","Epoch 75/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9892\n","Epoch 75: val_accuracy did not improve from 0.67317\n","1037/1037 [==============================] - 417s 402ms/step - loss: 0.0789 - accuracy: 0.9892 - val_loss: 2.3161 - val_accuracy: 0.6707 - lr: 1.4957e-05\n","Epoch 76/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9883\n","Epoch 76: ReduceLROnPlateau reducing learning rate to 1.353351126454072e-05.\n","\n","Epoch 76: val_accuracy did not improve from 0.67317\n","1037/1037 [==============================] - 414s 400ms/step - loss: 0.0804 - accuracy: 0.9883 - val_loss: 2.3694 - val_accuracy: 0.6715 - lr: 1.4957e-05\n","Epoch 77/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9895\n","Epoch 77: val_accuracy did not improve from 0.67317\n","1037/1037 [==============================] - 413s 399ms/step - loss: 0.0788 - accuracy: 0.9895 - val_loss: 2.3486 - val_accuracy: 0.6709 - lr: 1.3534e-05\n","Epoch 78/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9906\n","Epoch 78: val_accuracy improved from 0.67317 to 0.67902, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 415s 400ms/step - loss: 0.0766 - accuracy: 0.9906 - val_loss: 2.3349 - val_accuracy: 0.6790 - lr: 1.3534e-05\n","Epoch 79/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9902\n","Epoch 79: ReduceLROnPlateau reducing learning rate to 1.2245626749063376e-05.\n","\n","Epoch 79: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 414s 400ms/step - loss: 0.0757 - accuracy: 0.9902 - val_loss: 2.3744 - val_accuracy: 0.6729 - lr: 1.3534e-05\n","Epoch 80/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9910\n","Epoch 80: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 413s 399ms/step - loss: 0.0743 - accuracy: 0.9910 - val_loss: 2.3759 - val_accuracy: 0.6771 - lr: 1.2246e-05\n","Epoch 81/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9914\n","Epoch 81: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 415s 400ms/step - loss: 0.0709 - accuracy: 0.9914 - val_loss: 2.3299 - val_accuracy: 0.6679 - lr: 1.2246e-05\n","Epoch 82/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9916\n","Epoch 82: ReduceLROnPlateau reducing learning rate to 1.1080300282628741e-05.\n","\n","Epoch 82: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 414s 400ms/step - loss: 0.0717 - accuracy: 0.9916 - val_loss: 2.3097 - val_accuracy: 0.6698 - lr: 1.2246e-05\n","Epoch 83/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9917\n","Epoch 83: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 413s 399ms/step - loss: 0.0687 - accuracy: 0.9917 - val_loss: 2.3253 - val_accuracy: 0.6673 - lr: 1.1080e-05\n","Epoch 84/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9919\n","Epoch 84: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 415s 400ms/step - loss: 0.0684 - accuracy: 0.9919 - val_loss: 2.4006 - val_accuracy: 0.6673 - lr: 1.1080e-05\n","Epoch 85/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9914\n","Epoch 85: ReduceLROnPlateau reducing learning rate to 1.002586941467598e-05.\n","\n","Epoch 85: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 414s 399ms/step - loss: 0.0702 - accuracy: 0.9914 - val_loss: 2.4677 - val_accuracy: 0.6687 - lr: 1.1080e-05\n","Epoch 86/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9920\n","Epoch 86: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 415s 400ms/step - loss: 0.0677 - accuracy: 0.9920 - val_loss: 2.3899 - val_accuracy: 0.6723 - lr: 1.0026e-05\n","Epoch 87/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9922\n","Epoch 87: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 415s 400ms/step - loss: 0.0670 - accuracy: 0.9922 - val_loss: 2.4196 - val_accuracy: 0.6701 - lr: 1.0026e-05\n","Epoch 88/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9934\n","Epoch 88: ReduceLROnPlateau reducing learning rate to 9.071781278180424e-06.\n","\n","Epoch 88: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 414s 399ms/step - loss: 0.0630 - accuracy: 0.9934 - val_loss: 2.4371 - val_accuracy: 0.6746 - lr: 1.0026e-05\n","Epoch 89/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9932\n","Epoch 89: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 414s 400ms/step - loss: 0.0631 - accuracy: 0.9932 - val_loss: 2.4829 - val_accuracy: 0.6729 - lr: 9.0718e-06\n","Epoch 90/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9936\n","Epoch 90: val_accuracy did not improve from 0.67902\n","1037/1037 [==============================] - 414s 400ms/step - loss: 0.0619 - accuracy: 0.9936 - val_loss: 2.4989 - val_accuracy: 0.6695 - lr: 9.0718e-06\n","Epoch 91/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9934\n","Epoch 91: ReduceLROnPlateau reducing learning rate to 8.208487088268157e-06.\n","\n","Epoch 91: val_accuracy improved from 0.67902 to 0.68125, saving model to /content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/ferplus_vgg16_v1.hdf5\n","1037/1037 [==============================] - 417s 403ms/step - loss: 0.0600 - accuracy: 0.9934 - val_loss: 2.5194 - val_accuracy: 0.6812 - lr: 9.0718e-06\n","Epoch 92/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9939\n","Epoch 92: val_accuracy did not improve from 0.68125\n","1037/1037 [==============================] - 415s 400ms/step - loss: 0.0605 - accuracy: 0.9939 - val_loss: 2.5156 - val_accuracy: 0.6743 - lr: 8.2085e-06\n","Epoch 93/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9939\n","Epoch 93: val_accuracy did not improve from 0.68125\n","1037/1037 [==============================] - 416s 402ms/step - loss: 0.0604 - accuracy: 0.9939 - val_loss: 2.5322 - val_accuracy: 0.6723 - lr: 8.2085e-06\n","Epoch 94/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9943\n","Epoch 94: ReduceLROnPlateau reducing learning rate to 7.427345735777635e-06.\n","\n","Epoch 94: val_accuracy did not improve from 0.68125\n","1037/1037 [==============================] - 417s 402ms/step - loss: 0.0601 - accuracy: 0.9943 - val_loss: 2.4080 - val_accuracy: 0.6668 - lr: 8.2085e-06\n","Epoch 95/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9946\n","Epoch 95: val_accuracy did not improve from 0.68125\n","1037/1037 [==============================] - 416s 402ms/step - loss: 0.0590 - accuracy: 0.9946 - val_loss: 2.4014 - val_accuracy: 0.6726 - lr: 7.4273e-06\n","Epoch 96/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9945\n","Epoch 96: val_accuracy did not improve from 0.68125\n","1037/1037 [==============================] - 415s 401ms/step - loss: 0.0575 - accuracy: 0.9945 - val_loss: 2.4928 - val_accuracy: 0.6737 - lr: 7.4273e-06\n","Epoch 97/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9946\n","Epoch 97: ReduceLROnPlateau reducing learning rate to 6.72054011374712e-06.\n","\n","Epoch 97: val_accuracy did not improve from 0.68125\n","1037/1037 [==============================] - 416s 402ms/step - loss: 0.0580 - accuracy: 0.9946 - val_loss: 2.5599 - val_accuracy: 0.6762 - lr: 7.4273e-06\n","Epoch 98/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9942\n","Epoch 98: val_accuracy did not improve from 0.68125\n","1037/1037 [==============================] - 415s 401ms/step - loss: 0.0595 - accuracy: 0.9942 - val_loss: 2.4237 - val_accuracy: 0.6748 - lr: 6.7205e-06\n","Epoch 99/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9951\n","Epoch 99: val_accuracy did not improve from 0.68125\n","1037/1037 [==============================] - 417s 403ms/step - loss: 0.0557 - accuracy: 0.9951 - val_loss: 2.4908 - val_accuracy: 0.6760 - lr: 6.7205e-06\n","Epoch 100/100\n","1037/1037 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9948\n","Epoch 100: ReduceLROnPlateau reducing learning rate to 6.080995717638871e-06.\n","\n","Epoch 100: val_accuracy did not improve from 0.68125\n","1037/1037 [==============================] - 416s 401ms/step - loss: 0.0564 - accuracy: 0.9948 - val_loss: 2.4414 - val_accuracy: 0.6754 - lr: 6.7205e-06\n"]}],"source":["H = model.fit(train_ds,steps_per_epoch = 1037, validation_data=val_ds,\n","            epochs=config_train[\"epochs\"], verbose = 1, callbacks = [reduce_lr,model_checkpoint])\n"]},{"cell_type":"code","source":["model = get_models.Pre_train_model(VGG16, img_conc, None)\n","\n","# model.load_weights(\"/content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/FerPlusV11.hdf5\")\n","model.compile(loss =config_train[\"loss\"], optimizer =Adam(3e-4), metrics=[config_train[\"metric\"]])\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=tf.math.exp(-0.1),\n","                              patience=3, min_lr=0 , verbose =1)\n","\n","files = configfer[\"file_checkpoint\"]\n","checkpoint_filepath = (f\"{default_ws_dir}/checkpoints/{files}\")\n","\n","model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    verbose= 1,\n","    monitor='val_accuracy',\n","    mode='auto',\n","    save_best_only=True, save_weights_only = True)"],"metadata":{"id":"sFFAuPniArbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["H = model.fit(train_ds,steps_per_epoch = 1037, validation_data=val_ds, class_weight = class_weights,\n","            epochs=100, verbose = 1, callbacks = [reduce_lr,model_checkpoint])\n"],"metadata":{"id":"29qLUB1JAr8t","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1655955027322,"user_tz":-420,"elapsed":955,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"}},"outputId":"6177279b-1673-4755-f5ea-5971ca10417e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 224, 224, 3).\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-55edd6770d58>\", line 2, in <module>\n","    epochs=100, verbose = 1, callbacks = [reduce_lr,model_checkpoint])\n","  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n","    raise e.with_traceback(filtered_tb) from None\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n","    raise e.ag_error_metadata.to_exception(e)\n","ValueError: in user code:\n","\n","    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n","        return step_function(self, iterator)\n","    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n","        outputs = model.distribute_strategy.run(run_step, args=(data,))\n","    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n","        outputs = model.train_step(data)\n","    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n","        y_pred = self(x, training=True)\n","    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n","        raise e.with_traceback(filtered_tb) from None\n","    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 249, in assert_input_compatibility\n","        f'Input {input_index} of layer \"{layer_name}\" is '\n","\n","    ValueError: Exception encountered when calling layer \"model_1\" (type Functional).\n","    \n","    Input 0 of layer \"block1_conv1\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 224, 224, 9)\n","    \n","    Call arguments received:\n","      • inputs=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n","      • training=True\n","      • mask=None\n","\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"code","source":["# get_models.save_history(\"/content/drive/MyDrive/WorkSpace/AI_Research/Emotion/History/FerNew_V3.json\", H)"],"metadata":{"id":"hdNPoLAjfOoQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# def save_model(model,path)"],"metadata":{"id":"gi5yvc9GnGWR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluate"],"metadata":{"id":"SZJEn0yF1H4Q"}},{"cell_type":"code","source":["model = get_models.Pre_train_model1(ResNet50, img_conc, None)\n","model.compile(loss =\"categorical_crossentropy\", optimizer =Adamax(3e-7), metrics=[\"accuracy\"])\n","model.load_weights(\"/content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/FerCheckV6.hdf5\")"],"metadata":{"id":"sK9SJ8f5oZ60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.evaluate(test_ds)"],"metadata":{"id":"Djbm8zoHq1wN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images , test_labels = predict.get_imgs_test(test_ds)"],"metadata":{"id":"KzgtCMZlsprD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_models.Confusion_Matrix(model , images, test_labels, emotion , mode = True)"],"metadata":{"id":"S9AedYbfrOo_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKFQTApPAJTu"},"outputs":[],"source":["model = get_models.Pre_train_model1(VGG16, img_conc, None)\n","\n","# model.load_weights(\"/content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/FerCheckV6.hdf5\")\n","\n","model.compile(loss =\"categorical_crossentropy\", optimizer =Adamax(1e-4), metrics=[\"accuracy\"])\n","# EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n","                              patience=3, min_lr=0)\n","\n","checkpoint_filepath = \"/content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/FerNew_vper.hdf5\"\n","model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    verbose= 1,\n","    monitor='val_accuracy',\n","    mode='auto',\n","    save_best_only=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yABcLBTGw4hp"},"outputs":[],"source":["H = model.fit(train_ds, validation_data=val_ds, steps_per_epoch = 952*2,\n","              epochs=80, verbose = 1, callbacks = [reduce_lr,model_checkpoint ])"]},{"cell_type":"code","source":["get_models.save_history(\"/content/drive/MyDrive/WorkSpace/AI_Research/Emotion/History/FerNew_V2.json\", H)"],"metadata":{"id":"xzOQj9iL-Mg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"vYRCBTeas8xp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = get_models.Pre_train_model1(VGG16, img_conc, None)\n","model.summary()"],"metadata":{"id":"sIRTcAvAs8uC","executionInfo":{"status":"ok","timestamp":1659888501529,"user_tz":-420,"elapsed":1330,"user":{"displayName":"Hải Trần Minh","userId":"10829535699963954986"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8debc6ed-90bb-498b-f550-2f7adcf27bf9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 0s 0us/step\n","58900480/58889256 [==============================] - 0s 0us/step\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 224, 224, 1  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 224, 224, 3)  0           ['input_1[0][0]',                \n","                                                                  'input_1[0][0]',                \n","                                                                  'input_1[0][0]']                \n","                                                                                                  \n"," block1_conv1 (Conv2D)          (None, 224, 224, 64  1792        ['concatenate[0][0]']            \n","                                )                                                                 \n","                                                                                                  \n"," block1_conv2 (Conv2D)          (None, 224, 224, 64  36928       ['block1_conv1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," block1_pool (MaxPooling2D)     (None, 112, 112, 64  0           ['block1_conv2[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," block2_conv1 (Conv2D)          (None, 112, 112, 12  73856       ['block1_pool[0][0]']            \n","                                8)                                                                \n","                                                                                                  \n"," block2_conv2 (Conv2D)          (None, 112, 112, 12  147584      ['block2_conv1[0][0]']           \n","                                8)                                                                \n","                                                                                                  \n"," block2_pool (MaxPooling2D)     (None, 56, 56, 128)  0           ['block2_conv2[0][0]']           \n","                                                                                                  \n"," block3_conv1 (Conv2D)          (None, 56, 56, 256)  295168      ['block2_pool[0][0]']            \n","                                                                                                  \n"," block3_conv2 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv1[0][0]']           \n","                                                                                                  \n"," block3_conv3 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv2[0][0]']           \n","                                                                                                  \n"," block3_pool (MaxPooling2D)     (None, 28, 28, 256)  0           ['block3_conv3[0][0]']           \n","                                                                                                  \n"," block4_conv1 (Conv2D)          (None, 28, 28, 512)  1180160     ['block3_pool[0][0]']            \n","                                                                                                  \n"," block4_conv2 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv1[0][0]']           \n","                                                                                                  \n"," block4_conv3 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv2[0][0]']           \n","                                                                                                  \n"," block4_pool (MaxPooling2D)     (None, 14, 14, 512)  0           ['block4_conv3[0][0]']           \n","                                                                                                  \n"," block5_conv1 (Conv2D)          (None, 14, 14, 512)  2359808     ['block4_pool[0][0]']            \n","                                                                                                  \n"," block5_conv2 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv1[0][0]']           \n","                                                                                                  \n"," block5_conv3 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv2[0][0]']           \n","                                                                                                  \n"," block5_pool (MaxPooling2D)     (None, 7, 7, 512)    0           ['block5_conv3[0][0]']           \n","                                                                                                  \n"," global_average_pooling2d_1 (Gl  (None, 512)         0           ['block5_pool[0][0]']            \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," dense_5 (Dense)                (None, 1024)         525312      ['global_average_pooling2d_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," activation_4 (Activation)      (None, 1024)         0           ['dense_5[0][0]']                \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 1024)         0           ['activation_4[0][0]']           \n","                                                                                                  \n"," dense_6 (Dense)                (None, 256)          262400      ['dropout_4[0][0]']              \n","                                                                                                  \n"," activation_5 (Activation)      (None, 256)          0           ['dense_6[0][0]']                \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 256)          0           ['activation_5[0][0]']           \n","                                                                                                  \n"," dense_7 (Dense)                (None, 128)          32896       ['dropout_5[0][0]']              \n","                                                                                                  \n"," activation_6 (Activation)      (None, 128)          0           ['dense_7[0][0]']                \n","                                                                                                  \n"," dropout_6 (Dropout)            (None, 128)          0           ['activation_6[0][0]']           \n","                                                                                                  \n"," dense_8 (Dense)                (None, 64)           8256        ['dropout_6[0][0]']              \n","                                                                                                  \n"," activation_7 (Activation)      (None, 64)           0           ['dense_8[0][0]']                \n","                                                                                                  \n"," dropout_7 (Dropout)            (None, 64)           0           ['activation_7[0][0]']           \n","                                                                                                  \n"," dense_9 (Dense)                (None, 7)            455         ['dropout_7[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 15,544,007\n","Trainable params: 15,544,007\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"4u42rGkqs8qY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"GqFu8SRCs8mv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"IDs7Jkwrs8jQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3O15UEcxg7g"},"outputs":[],"source":["from keras.applications.vgg16 import VGG16\n","from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda\n","from keras.models import Model\n","from keras.layers import BatchNormalization\n","# in_lay = Input((128,128,3))\n","base_pretrained_model = VGG16(input_shape = None, input_tensor = img_conc ,\n","                              include_top = False, weights = 'imagenet')\n","input = base_pretrained_model.input \n","# base_pretrained_model.trainable = \n","\n","pt_depth = base_pretrained_model.layers[-1].output_shape[-1]\n","pt_features = base_pretrained_model.layers[-1].output\n","\n","bn_features = BatchNormalization()(pt_features)\n","\n","print(pt_features)\n","# here we do an attention mechanism to turn pixels in the GAP on an off\n","attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(bn_features)\n","attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n","attn_layer = Conv2D(1, kernel_size = (1,1), padding = 'valid', activation = 'sigmoid')(attn_layer)\n","\n","# fan it out to all of the channels\n","up_c2_w = np.ones((1, 1, 1, pt_depth))\n","up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n","               activation = 'linear', use_bias = False, weights = [up_c2_w])\n","up_c2.trainable = False\n","attn_layer = up_c2(attn_layer)\n","\n","mask_features = multiply([attn_layer, bn_features])\n","gap_features = GlobalAveragePooling2D()(mask_features)\n","gap_mask = GlobalAveragePooling2D()(attn_layer)\n","# to account for missing values from the attention model\n","\n","gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n","gap_dr = Dropout(0.5)(gap)\n","\n","dr_steps = Dropout(0.25)(gap_dr)\n","dr_steps = (Dense(512, activation = 'relu',  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n","                    bias_regularizer=regularizers.L2(1e-4),\n","                    activity_regularizer=regularizers.L2(1e-5)))(dr_steps)\n","dr_steps = Dropout(0.25)(dr_steps)\n","dr_steps = (Dense(128, activation = 'relu',  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n","                    bias_regularizer=regularizers.L2(1e-4),\n","                    activity_regularizer=regularizers.L2(1e-5)))(dr_steps)\n","\n","\n","out_layer = Dense(7, activation = 'softmax')(dr_steps)\n","\n","tb_model = Model(inputs = input, outputs = out_layer)\n","tb_model.summary()"]},{"cell_type":"code","source":["tb_model.compile(loss =\"categorical_crossentropy\", optimizer =Adamax(3e-4), metrics=[\"accuracy\"])\n","# EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n","                              patience=3, min_lr=0)\n","\n","checkpoint_filepath = \"/content/drive/MyDrive/WorkSpace/AI_Research/Emotion/checkpoints/FerGAPV1.hdf5\"\n","model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    verbose= 1,\n","    monitor='val_accuracy',\n","    mode='auto',\n","    save_best_only=True)"],"metadata":{"id":"emaEMINKmgix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["H = tb_model.fit(train_ds, validation_data=val_ds, \n","              epochs=80, verbose = 1, callbacks = [reduce_lr,model_checkpoint ])"],"metadata":{"id":"h1vcWjo2mvlh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"x-jPyGRIm8cQ"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Fer.ipynb","provenance":[{"file_id":"1lPhXXvE3G5PyhavAg18I3QziheRuAjjw","timestamp":1653145213831}],"authorship_tag":"ABX9TyOcbPcflY+p5N5ZBKdpFFMm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}